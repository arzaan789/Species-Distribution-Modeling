{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:56.891736Z",
     "start_time": "2025-08-04T23:51:56.452818Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "bat = pd.read_csv(\"bat_final_data.csv\")\n",
    "brown_hare = pd.read_csv(\"brown_hare_final_data.csv\")\n",
    "dormice = pd.read_csv(\"dormice_final_data.csv\")\n",
    "hedgehog = pd.read_csv(\"hedgehog_final_data.csv\")\n",
    "red_squirrel = pd.read_csv(\"red_squirrel_final_data.csv\")\n",
    "\n",
    "tgb_bat = pd.read_csv(\"tgb_bat_final_data.csv\")\n",
    "tgb_brown_hare = pd.read_csv(\"tgb_brown_hare_final_data.csv\")\n",
    "tgb_dormice = pd.read_csv(\"tgb_dormice_final_data.csv\")\n",
    "tgb_hedgehog = pd.read_csv(\"tgb_hedgehog_final_data.csv\")\n",
    "tgb_red_squirrel = pd.read_csv(\"tgb_red_squirrel_final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# drop na and duplicates\n",
    "bat = bat.dropna()\n",
    "bat = bat.drop_duplicates()\n",
    "brown_hare = brown_hare.dropna()\n",
    "brown_hare = brown_hare.drop_duplicates()\n",
    "dormice = dormice.dropna()\n",
    "dormice = dormice.drop_duplicates()\n",
    "hedgehog = hedgehog.dropna()\n",
    "hedgehog = hedgehog.drop_duplicates()\n",
    "red_squirrel = red_squirrel.dropna()\n",
    "red_squirrel = red_squirrel.drop_duplicates()\n",
    "\n",
    "tgb_bat = tgb_bat.dropna()\n",
    "tgb_brown_hare = tgb_brown_hare.dropna()\n",
    "tgb_dormice = tgb_dormice.dropna()\n",
    "tgb_hedgehog = tgb_hedgehog.dropna()\n",
    "tgb_red_squirrel = tgb_red_squirrel.dropna()\n",
    "\n",
    "tgb_bat = tgb_bat.drop_duplicates()\n",
    "tgb_brown_hare = tgb_brown_hare.drop_duplicates()\n",
    "tgb_dormice = tgb_dormice.drop_duplicates()\n",
    "tgb_hedgehog = tgb_hedgehog.drop_duplicates()\n",
    "tgb_red_squirrel = tgb_red_squirrel.drop_duplicates()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:57.042800Z",
     "start_time": "2025-08-04T23:51:56.889033Z"
    }
   },
   "id": "6e2fb50d8077974d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#drop date and species name\n",
    "bat = bat.drop(columns=['eventDate', 'species'])\n",
    "brown_hare = brown_hare.drop(columns=['eventDate', 'species'])\n",
    "dormice = dormice.drop(columns=['eventDate', 'species'])\n",
    "hedgehog = hedgehog.drop(columns=['eventDate', 'species'])\n",
    "red_squirrel = red_squirrel.drop(columns=['eventDate', 'species'])\n",
    "\n",
    "tgb_bat = tgb_bat.drop(columns=['eventDate', 'species'])\n",
    "tgb_brown_hare = tgb_brown_hare.drop(columns=['eventDate', 'species'])\n",
    "tgb_dormice = tgb_dormice.drop(columns=['eventDate', 'species'])\n",
    "tgb_hedgehog = tgb_hedgehog.drop(columns=['eventDate', 'species'])\n",
    "tgb_red_squirrel = tgb_red_squirrel.drop(columns=['eventDate', 'species'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:57.058308Z",
     "start_time": "2025-08-04T23:51:57.042125Z"
    }
   },
   "id": "dc01415436ee63cd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "bat['type'] = 'presence'\n",
    "brown_hare['type'] = 'presence'\n",
    "dormice['type'] = 'presence'\n",
    "hedgehog['type'] = 'presence'\n",
    "red_squirrel['type'] = 'presence'\n",
    "\n",
    "tgb_bat['type'] = 'background'\n",
    "tgb_brown_hare['type'] = 'background'\n",
    "tgb_dormice['type'] = 'background'\n",
    "tgb_hedgehog['type'] = 'background'\n",
    "tgb_red_squirrel['type'] = 'background'\n",
    "\n",
    "#append TGB data to species data\n",
    "bat = pd.concat([bat, tgb_bat], ignore_index=True)\n",
    "brown_hare = pd.concat([brown_hare, tgb_brown_hare], ignore_index=True)\n",
    "dormice = pd.concat([dormice, tgb_dormice], ignore_index=True)\n",
    "hedgehog = pd.concat([hedgehog, tgb_hedgehog], ignore_index=True)\n",
    "red_squirrel = pd.concat([red_squirrel, tgb_red_squirrel], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:57.083386Z",
     "start_time": "2025-08-04T23:51:57.058124Z"
    }
   },
   "id": "2c163af96b763f4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Bat region clustering\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(bat, geometry=gpd.points_from_xy(bat['decimalLongitude'], bat['decimalLatitude']), crs='EPSG:4326')\n",
    "\n",
    "utm_crs = CRS(\"EPSG:32630\")\n",
    "gdf_utm = gdf.to_crs(utm_crs)\n",
    "\n",
    "# Extract UTM X/Y for clustering\n",
    "coords_utm = gdf_utm.geometry.apply(lambda p: (p.x, p.y)).to_list()\n",
    "\n",
    "\n",
    "# HDBSCAN clustering\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,       # ensures even sparse regions form clusters\n",
    "    min_samples=1,\n",
    "    cluster_selection_epsilon=250.0,  # 250 meters\n",
    "    metric='euclidean'  # Now in meters, not degrees\n",
    ")\n",
    "\n",
    "# Fit and assign region IDs\n",
    "labels = clusterer.fit_predict(np.array(coords_utm))\n",
    "\n",
    "# Assuming `clusterer` is your trained HDBSCAN object\n",
    "with open('bat_region_clusterer.pkl', 'wb') as f:\n",
    "    pickle.dump(clusterer, f)\n",
    "\n",
    "\n",
    "# Assign unique cluster IDs to noise points (-1)\n",
    "noise_points = np.where(labels == -1)[0]\n",
    "max_label = labels.max()\n",
    "\n",
    "for i, idx in enumerate(noise_points):\n",
    "    labels[idx] = max_label + 1 + i\n",
    "\n",
    "gdf_utm['region_id'] = labels\n",
    "\n",
    "bat['region_id'] = gdf_utm['region_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:57.346420Z",
     "start_time": "2025-08-04T23:51:57.069737Z"
    }
   },
   "id": "f6f88f60e6f2018e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Brown hare region clustering\n",
    "gdf_brown_hare = gpd.GeoDataFrame(brown_hare, geometry=gpd.points_from_xy(brown_hare['decimalLongitude'], brown_hare['decimalLatitude']), crs='EPSG:4326')\n",
    "utm_crs_brown_hare = CRS(\"EPSG:32630\")\n",
    "gdf_brown_hare_utm = gdf_brown_hare.to_crs(utm_crs_brown_hare)\n",
    "# Extract UTM X/Y for clustering\n",
    "coords_utm_brown_hare = gdf_brown_hare_utm.geometry.apply(lambda p: (p.x, p.y)).to_list()\n",
    "# HDBSCAN clustering\n",
    "clusterer_brown_hare = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,       # ensures even sparse regions form clusters\n",
    "    min_samples=1,\n",
    "    cluster_selection_epsilon=250.0,  # 250 meters\n",
    "    metric='euclidean'  # Now in meters, not degrees\n",
    ")\n",
    "# Fit and assign region IDs\n",
    "labels_brown_hare = clusterer_brown_hare.fit_predict(np.array(coords_utm_brown_hare))\n",
    "# Assuming `clusterer_brown_hare` is your trained HDBSCAN object\n",
    "with open('brown_hare_region_clusterer.pkl', 'wb') as f:\n",
    "    pickle.dump(clusterer_brown_hare, f)\n",
    "# Assign unique cluster IDs to noise points (-1)\n",
    "noise_points_brown_hare = np.where(labels_brown_hare == -1)[0]\n",
    "max_label_brown_hare = labels_brown_hare.max()\n",
    "for i, idx in enumerate(noise_points_brown_hare):\n",
    "    labels_brown_hare[idx] = max_label_brown_hare + 1 + i\n",
    "gdf_brown_hare_utm['region_id'] = labels_brown_hare\n",
    "brown_hare['region_id'] = gdf_brown_hare_utm['region_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:59.088752Z",
     "start_time": "2025-08-04T23:51:57.348299Z"
    }
   },
   "id": "6f42badbbc421707"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dormice region clustering\n",
    "gdf_dormice = gpd.GeoDataFrame(dormice, geometry=gpd.points_from_xy(dormice['decimalLongitude'], dormice['decimalLatitude']), crs='EPSG:4326')\n",
    "utm_crs_dormice = CRS(\"EPSG:32630\")\n",
    "gdf_dormice_utm = gdf_dormice.to_crs(utm_crs_dormice)\n",
    "# Extract UTM X/Y for clustering\n",
    "coords_utm_dormice = gdf_dormice_utm.geometry.apply(lambda p: (p.x, p.y)).to_list()\n",
    "# HDBSCAN clustering\n",
    "clusterer_dormice = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,       # ensures even sparse regions form clusters\n",
    "    min_samples=1,\n",
    "    cluster_selection_epsilon=250.0,  # 250 meters\n",
    "    metric='euclidean'  # Now in meters, not degrees\n",
    ")\n",
    "# Fit and assign region IDs\n",
    "labels_dormice = clusterer_dormice.fit_predict(np.array(coords_utm_dormice))\n",
    "# Assuming `clusterer_dormice` is your trained HDBSCAN object\n",
    "with open('dormice_region_clusterer.pkl', 'wb') as f:\n",
    "    pickle.dump(clusterer_dormice, f)\n",
    "# Assign unique cluster IDs to noise points (-1)\n",
    "noise_points_dormice = np.where(labels_dormice == -1)[0]\n",
    "max_label_dormice = labels_dormice.max()\n",
    "for i, idx in enumerate(noise_points_dormice):\n",
    "    labels_dormice[idx] = max_label_dormice + 1 + i\n",
    "gdf_dormice_utm['region_id'] = labels_dormice\n",
    "dormice['region_id'] = gdf_dormice_utm['region_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:59.507358Z",
     "start_time": "2025-08-04T23:51:59.082868Z"
    }
   },
   "id": "506c970f981fe844"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hedgehog region clustering\n",
    "gdf_hedgehog = gpd.GeoDataFrame(hedgehog, geometry=gpd.points_from_xy(hedgehog['decimalLongitude'], hedgehog['decimalLatitude']), crs='EPSG:4326')\n",
    "utm_crs_hedgehog = CRS(\"EPSG:32630\")\n",
    "gdf_hedgehog_utm = gdf_hedgehog.to_crs(utm_crs_hedgehog)\n",
    "# Extract UTM X/Y for clustering\n",
    "coords_utm_hedgehog = gdf_hedgehog_utm.geometry.apply(lambda p: (p.x, p.y)).to_list()\n",
    "# HDBSCAN clustering\n",
    "clusterer_hedgehog = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,       # ensures even sparse regions form clusters\n",
    "    min_samples=1,\n",
    "    cluster_selection_epsilon=250.0,  # 250 meters\n",
    "    metric='euclidean'  # Now in meters, not degrees\n",
    ")\n",
    "# Fit and assign region IDs\n",
    "labels_hedgehog = clusterer_hedgehog.fit_predict(np.array(coords_utm_hedgehog))\n",
    "# Assuming `clusterer_hedgehog` is your trained HDBSCAN object\n",
    "with open('hedgehog_region_clusterer.pkl', 'wb') as f:\n",
    "    pickle.dump(clusterer_hedgehog, f)\n",
    "# Assign unique cluster IDs to noise points (-1)\n",
    "noise_points_hedgehog = np.where(labels_hedgehog == -1)[0]\n",
    "max_label_hedgehog = labels_hedgehog.max()\n",
    "for i, idx in enumerate(noise_points_hedgehog):\n",
    "    labels_hedgehog[idx] = max_label_hedgehog + 1 + i\n",
    "gdf_hedgehog_utm['region_id'] = labels_hedgehog\n",
    "hedgehog['region_id'] = gdf_hedgehog_utm['region_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:52:06.254109Z",
     "start_time": "2025-08-04T23:51:59.510Z"
    }
   },
   "id": "6a66e2ff4e33b419"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/arzaan/PycharmProjects/ComputationalIntelligence/venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Red squirrel region clustering\n",
    "gdf_red_squirrel = gpd.GeoDataFrame(red_squirrel, geometry=gpd.points_from_xy(red_squirrel['decimalLongitude'], red_squirrel['decimalLatitude']), crs='EPSG:4326')\n",
    "utm_crs_red_squirrel = CRS(\"EPSG:32630\")\n",
    "gdf_red_squirrel_utm = gdf_red_squirrel.to_crs(utm_crs_red_squirrel)\n",
    "# Extract UTM X/Y for clustering\n",
    "coords_utm_red_squirrel = gdf_red_squirrel_utm.geometry.apply(lambda p: (p.x, p.y)).to_list()\n",
    "# HDBSCAN clustering\n",
    "clusterer_red_squirrel = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,       # ensures even sparse regions form clusters\n",
    "    min_samples=1,\n",
    "    cluster_selection_epsilon=250.0,  # 250 meters\n",
    "    metric='euclidean'  # Now in meters, not degrees\n",
    ")\n",
    "# Fit and assign region IDs\n",
    "labels_red_squirrel = clusterer_red_squirrel.fit_predict(np.array(coords_utm_red_squirrel))\n",
    "# Assuming `clusterer_red_squirrel` is your trained HDBSCAN object\n",
    "with open('red_squirrel_region_clusterer.pkl', 'wb') as f:\n",
    "    pickle.dump(clusterer_red_squirrel, f)\n",
    "# Assign unique cluster IDs to noise points (-1)\n",
    "noise_points_red_squirrel = np.where(labels_red_squirrel == -1)[0]\n",
    "max_label_red_squirrel = labels_red_squirrel.max()\n",
    "for i, idx in enumerate(noise_points_red_squirrel):\n",
    "    labels_red_squirrel[idx] = max_label_red_squirrel + 1 + i\n",
    "gdf_red_squirrel_utm['region_id'] = labels_red_squirrel\n",
    "red_squirrel['region_id'] = gdf_red_squirrel_utm['region_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:52:11.275817Z",
     "start_time": "2025-08-04T23:52:06.256110Z"
    }
   },
   "id": "408ccaac4c2992c2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# drop lat/long columns\n",
    "bat = bat.drop(columns=['decimalLatitude', 'decimalLongitude'])\n",
    "brown_hare = brown_hare.drop(columns=['decimalLatitude', 'decimalLongitude'])\n",
    "dormice = dormice.drop(columns=['decimalLatitude', 'decimalLongitude'])\n",
    "hedgehog = hedgehog.drop(columns=['decimalLatitude', 'decimalLongitude'])\n",
    "red_squirrel = red_squirrel.drop(columns=['decimalLatitude', 'decimalLongitude'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:51:52.441883Z",
     "start_time": "2025-08-04T23:51:52.426851Z"
    }
   },
   "id": "5a6cb05376081409"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "bat_bool_columns = ['ceratopogonidae_presence','chironomidae_presence','cats_presence','wind_turbines_presence','near_road']\n",
    "dormice_bool_columns = ['hazel_presence','birch_presence','beech_presence','honeysuckle_presence','oak_presence','hawthorn_presence']\n",
    "hedgehog_bool_columns = ['badger_presence','ground_beetles_presence','near_road']\n",
    "brown_hare_bool_columns = ['winter_wheat_presence']\n",
    "red_squirrel_bool_columns = ['sitka_spruce_presence','grey_squirrel_presence']\n",
    "\n",
    "bat[bat_bool_columns] = bat[bat_bool_columns].astype(bool)\n",
    "dormice[dormice_bool_columns] = dormice[dormice_bool_columns].astype(bool)\n",
    "hedgehog[hedgehog_bool_columns] = hedgehog[hedgehog_bool_columns].astype(bool)\n",
    "brown_hare[brown_hare_bool_columns] = brown_hare[brown_hare_bool_columns].astype(bool)\n",
    "red_squirrel[red_squirrel_bool_columns] = red_squirrel[red_squirrel_bool_columns].astype(bool)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:52:11.309881Z",
     "start_time": "2025-08-04T23:52:11.283917Z"
    }
   },
   "id": "45d3acf7b5b51146"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "bat = pd.get_dummies(bat, columns=['Land_cover'])\n",
    "brown_hare = pd.get_dummies(brown_hare, columns=['Land_cover'])\n",
    "dormice = pd.get_dummies(dormice, columns=['Land_cover'])\n",
    "hedgehog = pd.get_dummies(hedgehog, columns=['Land_cover'])\n",
    "red_squirrel = pd.get_dummies(red_squirrel, columns=['Land_cover'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:52:17.842380Z",
     "start_time": "2025-08-04T23:52:17.782620Z"
    }
   },
   "id": "6a2388619d51d57d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# save the dataframes\n",
    "bat.to_csv('bat_final_data_preprocessed.csv', index=False)\n",
    "brown_hare.to_csv('brown_hare_final_data_preprocessed.csv', index=False)\n",
    "dormice.to_csv('dormice_final_data_preprocessed.csv', index=False)\n",
    "hedgehog.to_csv('hedgehog_final_data_preprocessed.csv', index=False)\n",
    "red_squirrel.to_csv('red_squirrel_final_data_preprocessed.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-21T23:38:58.994480Z",
     "start_time": "2025-07-21T23:38:57.868403Z"
    }
   },
   "id": "5e499aa7ede0c614"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# # save the dataframes\n",
    "# bat.to_csv('bat_final_data_preprocessed_with_coords.csv', index=False)\n",
    "# brown_hare.to_csv('brown_hare_final_data_preprocessed_with_coords.csv', index=False)\n",
    "# dormice.to_csv('dormice_final_data_preprocessed_with_coords.csv', index=False)\n",
    "# hedgehog.to_csv('hedgehog_final_data_preprocessed_with_coords.csv', index=False)\n",
    "# red_squirrel.to_csv('red_squirrel_final_data_preprocessed_with_coords.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-04T23:52:23.901792Z",
     "start_time": "2025-08-04T23:52:20.388272Z"
    }
   },
   "id": "2bf40de523aaab2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f60129d7869aa6bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
